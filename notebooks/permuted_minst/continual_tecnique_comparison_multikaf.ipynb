{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath('/media/jary/DATA/Uni/tesi/codice')\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import NoKafnet, Kafnet\n",
    "import utils.datasetsUtils.MINST as MINST\n",
    "from utils.datasetsUtils.taskManager import SingleTargetClassificationTask, NoTask\n",
    "import configs.configClasses as configClasses\n",
    "from torchvision.transforms import transforms\n",
    "import torch\n",
    "import networks.continual_learning as continual_learning\n",
    "import networks.continual_learning_beta as continual_learning_beta\n",
    "\n",
    "from Trainer import Trainer\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "GeForce GTX 1050\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG PARAMETERS\n",
      "BATCH_SIZE: 64\n",
      "DEVICE: cuda\n",
      "EPOCHS: 20\n",
      "EWC_IMPORTANCE: 1000\n",
      "EWC_SAMPLE_SIZE: 250\n",
      "EWC_TYPE: <class 'networks.continual_learning.OnlineEWC'>\n",
      "GAMMA: 1.0\n",
      "IS_CONVOLUTIONAL: False\n",
      "ITERS: 1\n",
      "L1_REG: 0\n",
      "LOSS: cross_entropy\n",
      "LR: 0.001\n",
      "MODEL_NAME: \n",
      "OPTIMIZER: SGD\n",
      "RUN_NAME: default\n",
      "SAVE_PATH: ./models/tecnique_comparison_multikaf\n",
      "USE_EWC: True\n",
      "USE_TENSORBOARD: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "configOnline = configClasses.OnlineLearningConfig()\n",
    "configOnline.L1_REG = 0\n",
    "configOnline.EPOCHS = 20\n",
    "configOnline.SAVE_PATH = './models/tecnique_comparison_multikaf'\n",
    "configOnline.IS_CONVOLUTIONAL = False\n",
    "configOnline.MODEL_NAME = ''\n",
    "print(configOnline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/minst/download\n",
      "task #0 with train 56000 and test 14000 images (label: 0)\n",
      "task #1 with train 56000 and test 14000 images (label: 1)\n",
      "task #2 with train 56000 and test 14000 images (label: 2)\n",
      "task #3 with train 56000 and test 14000 images (label: 3)\n"
     ]
    }
   ],
   "source": [
    "dataset = MINST.PermutedMINST('../data/minst', download=True, n_permutation=4,\n",
    "                        force_download=False, train_split=0.8, transform=None, target_transform=None)\n",
    "dataset.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = multikaf = Kafnet.MultiKAFMLP(len(dataset.class_to_idx), hidden_size=int(400*0.7),  kaf_init_fcn=None,\n",
    "                             trainable_dict=False, kernel_combination='sum', kernels=['gaussian', 'sigmoid', 'softplus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "experiments = [('no_cont_learning', None), \n",
    "               ('ewc', continual_learning.OnlineEWC),\n",
    "#                ('gem', continual_learning.GEM),\n",
    "               ('embedding', continual_learning_beta.embedding)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_cont_learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/jary/DATA/Uni/tesi/codice/Trainer.py:36: UserWarning: Ewc type is set to None  \n",
      "  warnings.warn(\"Ewc type is set to None  \")\n",
      "Training task 0, epoch 1: 100%|██████████| 875/875 [00:17<00:00, 50.66it/s, loss=2.13, batch#=875]\n",
      "Testing task 0: 219it [00:03, 71.38it/s, batch#=219]\n",
      "Training task 0, epoch 2: 100%|██████████| 875/875 [00:16<00:00, 52.67it/s, loss=1.45, batch#=875]\n",
      "Testing task 0: 219it [00:03, 70.39it/s, batch#=219]\n",
      "Training task 0, epoch 3: 100%|██████████| 875/875 [00:17<00:00, 50.50it/s, loss=0.873, batch#=875]\n",
      "Testing task 0: 219it [00:03, 68.90it/s, batch#=219]\n",
      "Training task 0, epoch 4: 100%|██████████| 875/875 [00:16<00:00, 51.95it/s, loss=0.62, batch#=875] \n",
      "Testing task 0: 219it [00:03, 69.51it/s, batch#=219]\n",
      "Training task 0, epoch 5: 100%|██████████| 875/875 [00:16<00:00, 51.65it/s, loss=0.493, batch#=875]\n",
      "Testing task 0: 219it [00:03, 69.16it/s, batch#=219]\n",
      "Training task 0, epoch 6: 100%|██████████| 875/875 [00:16<00:00, 50.15it/s, loss=0.426, batch#=875]\n",
      "Testing task 0: 219it [00:03, 68.86it/s, batch#=219]\n",
      "Training task 0, epoch 7: 100%|██████████| 875/875 [00:16<00:00, 51.63it/s, loss=0.385, batch#=875]\n",
      "Testing task 0: 219it [00:03, 69.40it/s, batch#=219]\n",
      "Training task 0, epoch 8: 100%|██████████| 875/875 [00:16<00:00, 51.91it/s, loss=0.358, batch#=875]\n",
      "Testing task 0: 219it [00:03, 69.78it/s, batch#=219]\n",
      "Training task 0, epoch 9: 100%|██████████| 875/875 [00:16<00:00, 50.56it/s, loss=0.335, batch#=875]\n",
      "Testing task 0: 219it [00:03, 69.55it/s, batch#=219]\n",
      "Training task 0, epoch 10: 100%|██████████| 875/875 [00:17<00:00, 51.42it/s, loss=0.317, batch#=875]\n",
      "Testing task 0: 219it [00:03, 68.20it/s, batch#=219]\n",
      "Training task 0, epoch 11: 100%|██████████| 875/875 [00:16<00:00, 51.51it/s, loss=0.303, batch#=875]\n",
      "Testing task 0: 219it [00:03, 69.31it/s, batch#=219]\n",
      "Training task 0, epoch 12: 100%|██████████| 875/875 [00:16<00:00, 51.82it/s, loss=0.29, batch#=875] \n",
      "Testing task 0: 219it [00:03, 69.42it/s, batch#=219]\n",
      "Training task 0, epoch 13: 100%|██████████| 875/875 [00:17<00:00, 51.45it/s, loss=0.279, batch#=875]\n",
      "Testing task 0: 219it [00:03, 69.44it/s, batch#=219]\n",
      "Training task 0, epoch 14: 100%|██████████| 875/875 [00:17<00:00, 47.32it/s, loss=0.269, batch#=875]\n",
      "Testing task 0: 219it [00:03, 66.07it/s, batch#=219]\n",
      "Training task 0, epoch 15: 100%|██████████| 875/875 [00:17<00:00, 50.21it/s, loss=0.261, batch#=875]\n",
      "Testing task 0: 219it [00:03, 64.37it/s, batch#=219]\n",
      "Training task 0, epoch 16: 100%|██████████| 875/875 [00:17<00:00, 49.61it/s, loss=0.251, batch#=875]\n",
      "Testing task 0: 219it [00:03, 69.36it/s, batch#=219]\n",
      "Training task 0, epoch 17: 100%|██████████| 875/875 [00:17<00:00, 52.13it/s, loss=0.243, batch#=875]\n",
      "Testing task 0: 219it [00:03, 69.68it/s, batch#=219]\n",
      "Training task 0, epoch 18: 100%|██████████| 875/875 [00:16<00:00, 51.96it/s, loss=0.235, batch#=875]\n",
      "Testing task 0: 219it [00:03, 69.68it/s, batch#=219]\n",
      "Training task 0, epoch 19: 100%|██████████| 875/875 [00:16<00:00, 51.56it/s, loss=0.228, batch#=875]\n",
      "Testing task 0: 219it [00:03, 69.35it/s, batch#=219]\n",
      "Training task 0, epoch 20: 100%|██████████| 875/875 [00:16<00:00, 52.30it/s, loss=0.222, batch#=875]\n",
      "Testing task 0: 219it [00:03, 69.99it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 69.62it/s, batch#=219]\n",
      "Testing task 2: 219it [00:03, 66.44it/s, batch#=219]\n",
      "Testing task 3: 219it [00:03, 63.60it/s, batch#=219]\n",
      "Training task 1, epoch 1: 100%|██████████| 875/875 [00:17<00:00, 50.52it/s, loss=0.799, batch#=875]\n",
      "Testing task 1: 219it [00:03, 67.29it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 63.74it/s, batch#=219]\n",
      "Training task 1, epoch 2: 100%|██████████| 875/875 [00:19<00:00, 44.59it/s, loss=0.401, batch#=875]\n",
      "Testing task 1: 219it [00:03, 59.94it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 60.08it/s, batch#=219]\n",
      "Training task 1, epoch 3: 100%|██████████| 875/875 [00:19<00:00, 45.71it/s, loss=0.343, batch#=875]\n",
      "Testing task 1: 219it [00:03, 57.83it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 59.48it/s, batch#=219]\n",
      "Training task 1, epoch 4: 100%|██████████| 875/875 [00:19<00:00, 44.77it/s, loss=0.312, batch#=875]\n",
      "Testing task 1: 219it [00:03, 60.08it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 60.08it/s, batch#=219]\n",
      "Training task 1, epoch 5: 100%|██████████| 875/875 [00:19<00:00, 44.93it/s, loss=0.291, batch#=875]\n",
      "Testing task 1: 219it [00:03, 59.96it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 60.00it/s, batch#=219]\n",
      "Training task 1, epoch 6: 100%|██████████| 875/875 [00:19<00:00, 44.99it/s, loss=0.275, batch#=875]\n",
      "Testing task 1: 219it [00:03, 59.62it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 59.32it/s, batch#=219]\n",
      "Training task 1, epoch 7: 100%|██████████| 875/875 [00:19<00:00, 43.16it/s, loss=0.261, batch#=875]\n",
      "Testing task 1: 219it [00:03, 58.27it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 60.79it/s, batch#=219]\n",
      "Training task 1, epoch 8: 100%|██████████| 875/875 [00:19<00:00, 43.73it/s, loss=0.249, batch#=875]\n",
      "Testing task 1: 219it [00:03, 57.61it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 57.33it/s, batch#=219]\n",
      "Training task 1, epoch 9: 100%|██████████| 875/875 [00:18<00:00, 46.17it/s, loss=0.238, batch#=875]\n",
      "Testing task 1: 219it [00:03, 60.24it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 62.33it/s, batch#=219]\n",
      "Training task 1, epoch 10: 100%|██████████| 875/875 [00:18<00:00, 45.00it/s, loss=0.229, batch#=875]\n",
      "Testing task 1: 219it [00:03, 58.07it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 59.99it/s, batch#=219]\n",
      "Training task 1, epoch 11: 100%|██████████| 875/875 [00:18<00:00, 46.10it/s, loss=0.22, batch#=875] \n",
      "Testing task 1: 219it [00:03, 60.60it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 61.18it/s, batch#=219]\n",
      "Training task 1, epoch 12: 100%|██████████| 875/875 [00:18<00:00, 46.20it/s, loss=0.213, batch#=875]\n",
      "Testing task 1: 219it [00:03, 61.06it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 61.09it/s, batch#=219]\n",
      "Training task 1, epoch 13: 100%|██████████| 875/875 [00:18<00:00, 46.89it/s, loss=0.205, batch#=875]\n",
      "Testing task 1: 219it [00:03, 60.92it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 61.45it/s, batch#=219]\n",
      "Training task 1, epoch 14: 100%|██████████| 875/875 [00:18<00:00, 45.84it/s, loss=0.2, batch#=875]  \n",
      "Testing task 1: 219it [00:03, 60.73it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 60.87it/s, batch#=219]\n",
      "Training task 1, epoch 15: 100%|██████████| 875/875 [00:19<00:00, 44.90it/s, loss=0.194, batch#=875]\n",
      "Testing task 1: 219it [00:03, 60.34it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 61.84it/s, batch#=219]\n",
      "Training task 1, epoch 16: 100%|██████████| 875/875 [00:18<00:00, 46.12it/s, loss=0.188, batch#=875]\n",
      "Testing task 1: 219it [00:03, 60.25it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 60.96it/s, batch#=219]\n",
      "Training task 1, epoch 17: 100%|██████████| 875/875 [00:18<00:00, 46.09it/s, loss=0.183, batch#=875]\n",
      "Testing task 1: 219it [00:03, 61.48it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 61.38it/s, batch#=219]\n",
      "Training task 1, epoch 18: 100%|██████████| 875/875 [00:18<00:00, 46.78it/s, loss=0.178, batch#=875]\n",
      "Testing task 1: 219it [00:03, 60.77it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 61.52it/s, batch#=219]\n",
      "Training task 1, epoch 19: 100%|██████████| 875/875 [00:18<00:00, 45.90it/s, loss=0.173, batch#=875]\n",
      "Testing task 1: 219it [00:03, 61.33it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 60.55it/s, batch#=219]\n",
      "Training task 1, epoch 20: 100%|██████████| 875/875 [00:18<00:00, 46.64it/s, loss=0.169, batch#=875]\n",
      "Testing task 1: 219it [00:03, 61.02it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 61.72it/s, batch#=219]\n",
      "Testing task 2: 219it [00:03, 61.42it/s, batch#=219]\n",
      "Testing task 3: 219it [00:03, 60.51it/s, batch#=219]\n",
      "Training task 2, epoch 1: 100%|██████████| 875/875 [00:18<00:00, 46.85it/s, loss=0.709, batch#=875]\n",
      "Testing task 2: 219it [00:03, 61.31it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 61.47it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 60.90it/s, batch#=219]\n",
      "Training task 2, epoch 2: 100%|██████████| 875/875 [00:18<00:00, 46.30it/s, loss=0.352, batch#=875]\n",
      "Testing task 2: 219it [00:03, 60.53it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 61.81it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 61.82it/s, batch#=219]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training task 2, epoch 3: 100%|██████████| 875/875 [00:18<00:00, 46.05it/s, loss=0.302, batch#=875]\n",
      "Testing task 2: 219it [00:03, 60.56it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 61.37it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 61.61it/s, batch#=219]\n",
      "Training task 2, epoch 4: 100%|██████████| 875/875 [00:18<00:00, 45.69it/s, loss=0.276, batch#=875]\n",
      "Testing task 2: 219it [00:03, 60.79it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 61.38it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 61.27it/s, batch#=219]\n",
      "Training task 2, epoch 5: 100%|██████████| 875/875 [00:18<00:00, 46.16it/s, loss=0.255, batch#=875]\n",
      "Testing task 2: 219it [00:03, 60.36it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 61.10it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 60.59it/s, batch#=219]\n",
      "Training task 2, epoch 6: 100%|██████████| 875/875 [00:18<00:00, 46.41it/s, loss=0.239, batch#=875]\n",
      "Testing task 2: 219it [00:03, 61.51it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 62.02it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 62.85it/s, batch#=219]\n",
      "Training task 2, epoch 7: 100%|██████████| 875/875 [00:17<00:00, 51.89it/s, loss=0.227, batch#=875]\n",
      "Testing task 2: 219it [00:03, 68.26it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 68.31it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 67.03it/s, batch#=219]\n",
      "Training task 2, epoch 8: 100%|██████████| 875/875 [00:17<00:00, 51.82it/s, loss=0.216, batch#=875]\n",
      "Testing task 2: 219it [00:03, 68.14it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 67.09it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 66.36it/s, batch#=219]\n",
      "Training task 2, epoch 9: 100%|██████████| 875/875 [00:18<00:00, 47.56it/s, loss=0.207, batch#=875]\n",
      "Testing task 2: 219it [00:03, 61.13it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 61.72it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 61.40it/s, batch#=219]\n",
      "Training task 2, epoch 10: 100%|██████████| 875/875 [00:18<00:00, 46.23it/s, loss=0.199, batch#=875]\n",
      "Testing task 2: 219it [00:03, 61.03it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 61.30it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 61.74it/s, batch#=219]\n",
      "Training task 2, epoch 11: 100%|██████████| 875/875 [00:18<00:00, 45.42it/s, loss=0.192, batch#=875]\n",
      "Testing task 2: 219it [00:03, 60.87it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 61.93it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 61.24it/s, batch#=219]\n",
      "Training task 2, epoch 12: 100%|██████████| 875/875 [00:18<00:00, 45.74it/s, loss=0.186, batch#=875]\n",
      "Testing task 2: 219it [00:03, 61.93it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 60.77it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 61.47it/s, batch#=219]\n",
      "Training task 2, epoch 13: 100%|██████████| 875/875 [00:18<00:00, 46.62it/s, loss=0.179, batch#=875]\n",
      "Testing task 2: 219it [00:03, 61.89it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 61.28it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 60.13it/s, batch#=219]\n",
      "Training task 2, epoch 14: 100%|██████████| 875/875 [00:16<00:00, 51.48it/s, loss=0.175, batch#=875]\n",
      "Testing task 2: 219it [00:03, 70.70it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 67.34it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 65.11it/s, batch#=219]\n",
      "Training task 2, epoch 15: 100%|██████████| 875/875 [00:17<00:00, 46.00it/s, loss=0.169, batch#=875]\n",
      "Testing task 2: 219it [00:03, 71.70it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 68.70it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 67.45it/s, batch#=219]\n",
      "Training task 2, epoch 16: 100%|██████████| 875/875 [00:17<00:00, 49.87it/s, loss=0.165, batch#=875]\n",
      "Testing task 2: 219it [00:03, 68.87it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 64.28it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 66.74it/s, batch#=219]\n",
      "Training task 2, epoch 17: 100%|██████████| 875/875 [00:17<00:00, 50.60it/s, loss=0.161, batch#=875]\n",
      "Testing task 2: 219it [00:03, 71.13it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 71.43it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 71.33it/s, batch#=219]\n",
      "Training task 2, epoch 18: 100%|██████████| 875/875 [00:16<00:00, 52.92it/s, loss=0.158, batch#=875]\n",
      "Testing task 2: 219it [00:03, 70.98it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 71.54it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 71.27it/s, batch#=219]\n",
      "Training task 2, epoch 19: 100%|██████████| 875/875 [00:16<00:00, 52.78it/s, loss=0.153, batch#=875]\n",
      "Testing task 2: 219it [00:03, 71.14it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 71.49it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 71.91it/s, batch#=219]\n",
      "Training task 2, epoch 20: 100%|██████████| 875/875 [00:16<00:00, 52.88it/s, loss=0.15, batch#=875] \n",
      "Testing task 2: 219it [00:03, 71.11it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 71.68it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 71.91it/s, batch#=219]\n",
      "Testing task 3: 219it [00:03, 71.02it/s, batch#=219]\n",
      "Training task 3, epoch 1: 100%|██████████| 875/875 [00:16<00:00, 52.90it/s, loss=0.753, batch#=875]\n",
      "Testing task 3: 219it [00:03, 71.76it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 71.22it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 71.21it/s, batch#=219]\n",
      "Testing task 2: 219it [00:03, 71.15it/s, batch#=219]\n",
      "Training task 3, epoch 2: 100%|██████████| 875/875 [00:16<00:00, 52.74it/s, loss=0.367, batch#=875]\n",
      "Testing task 3: 219it [00:03, 71.18it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 71.49it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 71.97it/s, batch#=219]\n",
      "Testing task 2: 219it [00:03, 71.32it/s, batch#=219]\n",
      "Training task 3, epoch 3: 100%|██████████| 875/875 [00:16<00:00, 52.88it/s, loss=0.307, batch#=875]\n",
      "Testing task 3: 219it [00:03, 71.13it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 71.37it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 71.23it/s, batch#=219]\n",
      "Testing task 2: 219it [00:03, 70.97it/s, batch#=219]\n",
      "Training task 3, epoch 4: 100%|██████████| 875/875 [00:16<00:00, 52.85it/s, loss=0.274, batch#=875]\n",
      "Testing task 3: 219it [00:03, 70.91it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 70.77it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 71.32it/s, batch#=219]\n",
      "Testing task 2: 219it [00:03, 71.86it/s, batch#=219]\n",
      "Training task 3, epoch 5: 100%|██████████| 875/875 [00:16<00:00, 53.25it/s, loss=0.25, batch#=875] \n",
      "Testing task 3: 219it [00:03, 72.05it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 72.09it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 71.61it/s, batch#=219]\n",
      "Testing task 2: 219it [00:03, 71.36it/s, batch#=219]\n",
      "Training task 3, epoch 6: 100%|██████████| 875/875 [00:16<00:00, 52.85it/s, loss=0.234, batch#=875]\n",
      "Testing task 3: 219it [00:03, 71.14it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 71.31it/s, batch#=219]\n",
      "Testing task 1: 219it [00:03, 70.87it/s, batch#=219]\n",
      "Testing task 2: 219it [00:03, 64.78it/s, batch#=219]\n",
      "Training task 3, epoch 7: 100%|██████████| 875/875 [00:17<00:00, 51.24it/s, loss=0.22, batch#=875] \n",
      "Testing task 3: 219it [00:03, 68.48it/s, batch#=219]\n",
      "Testing task 0: 219it [00:03, 68.51it/s, batch#=219]\n",
      "Testing task 1: 147it [00:02, 66.61it/s, batch#=147]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-75b5dd84753f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_tasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/jary/DATA/Uni/tesi/codice/Trainer.py\u001b[0m in \u001b[0;36mall_tasks\u001b[0;34m(self, limit)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcurrent_task\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks_number\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0msub_task\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_task\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_task\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;31m# for t, v in self.metrics_calculator.metrics['tasks'].items():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/jary/DATA/Uni/tesi/codice/Trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, evaluated_task)\u001b[0m\n\u001b[1;32m    212\u001b[0m                    \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0my_pred_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0my_true_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/jary/DATA/Uni/tesi/codice/networks/Kafnet.py\u001b[0m in \u001b[0;36meval_forward\u001b[0;34m(self, x, task)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for name, tec in experiments:\n",
    "    print(name)\n",
    "    \n",
    "    n = deepcopy(net)\n",
    "    config = deepcopy(configOnline)\n",
    "    \n",
    "    config.MODEL_NAME = name\n",
    "\n",
    "    if 'name' == 'gem':\n",
    "        config.EWC_IMPORTANCE = 0.5\n",
    "        \n",
    "    config.EWC_TYPE = tec\n",
    "    \n",
    "    trainer = Trainer(n, deepcopy(dataset), config, save_modality=2)\n",
    "\n",
    "    r = trainer.load()\n",
    "    if not r:\n",
    "        r = trainer.all_tasks()\n",
    "    \n",
    "    results.append((name, r))\n",
    "    \n",
    "    del trainer\n",
    "    del n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RESULTS')\n",
    "for name, r in results:\n",
    "    print(name, r['metrics'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_task = len(results[0][1]['tasks'])\n",
    "\n",
    "tot_epochs = 0\n",
    "\n",
    "for _, r in results:\n",
    "    for k, v in r['tasks'].items():\n",
    "        tot_epochs = max(tot_epochs, len(v['accuracy']))\n",
    "      \n",
    "fig, ax = plt.subplots(nrows=n_task, ncols=1, figsize=(22, 24), sharex=True, sharey=True)\n",
    "\n",
    "for name, r in results:\n",
    "    for i, task in enumerate(r['tasks'].keys()):\n",
    "\n",
    "        com = r['tasks'][task]\n",
    "        #no_ewt = metrics_no_ewt['tasks'][task]\n",
    "\n",
    "        x = range(tot_epochs-len(com['accuracy']), tot_epochs)\n",
    "\n",
    "        #ax = fig.add_subplot(n_task, 1, i+1, sharex=ax) \n",
    "\n",
    "        ax[i].plot(x, com['accuracy'], label=name)\n",
    "        #ax.plot(x, no_ewt['accuracy'], label='online ewt')\n",
    "\n",
    "        ax[i].set_xticks(range(0, tot_epochs, 5),minor=False)\n",
    "\n",
    "        ax[i].set_title(\"Task {}\".format(task))\n",
    "        ax[i].legend(loc=\"lower left\")\n",
    "        ax[i].grid(True, axis='both')\n",
    "\n",
    "        \n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, tec in experiments:\n",
    "\n",
    "    current_w = {n:p.cpu() for n, p in net.named_parameters() if p.requires_grad}\n",
    "\n",
    "    x = np.arange(4)\n",
    "    y = np.arange(len(current_w))\n",
    "    \n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    z = np.zeros(shape=(len(x), len(y)))\n",
    "    \n",
    "    fig = plt.figure(figsize=(22, 22))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    x_labels = []\n",
    "    \n",
    "    for i in x:\n",
    "\n",
    "        if i == 0:\n",
    "            x_labels.append('0')\n",
    "            \n",
    "        else:\n",
    "            x_labels.append('{}->{}'.format(i-1, i))\n",
    "            \n",
    "        n = deepcopy(net)\n",
    "        config = deepcopy(configOnline)\n",
    "\n",
    "        config.MODEL_NAME = name\n",
    "\n",
    "        if 'name' == 'gem':\n",
    "            config.EWC_IMPORTANCE = 0.5\n",
    "\n",
    "        config.EWC_TYPE = tec\n",
    "\n",
    "        trainer = Trainer(n, deepcopy(dataset), config, save_modality=2)\n",
    "\n",
    "        r = trainer.load(i)\n",
    "        \n",
    "        diff = {n: torch.dist(p.cpu(), current_w[n]) for n, p in trainer.model.named_parameters() if p.requires_grad }\n",
    "        current_w = {n: p.cpu() for n, p in trainer.model.named_parameters() if p.requires_grad }\n",
    "        \n",
    "        for j, (_, d) in enumerate(diff.items()):\n",
    "            z[i, j] = d\n",
    "    \n",
    "\n",
    "    z_n = z.flatten()\n",
    "\n",
    "    ax.view_init(45, 45)\n",
    "    \n",
    "    ax.bar3d(xx.flatten(),\n",
    "          yy.flatten(),\n",
    "          np.zeros(len(z_n)),\n",
    "          0.2, 0.5, z_n )\n",
    "    \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(x_labels)\n",
    "    \n",
    "    ax.set_yticks(y)\n",
    "    ax.set_yticklabels(list(diff.keys()), fontdict={'fontsize':12})\n",
    "    \n",
    "    ax.text(3.5, 0, 2, name.upper(), color='black',  fontsize='xx-large', \n",
    "        bbox=dict(facecolor='white', edgecolor='black'))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tot_epochs = 0\n",
    "\n",
    "fig, ax = plt.subplots(nrows=len(experiments), ncols=4, figsize=(35, 40), \n",
    "                       sharex=True, sharey=True,  subplot_kw={'projection':'3d'})\n",
    "\n",
    "for k in range(4):\n",
    "    dataset.task = k\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for im in range(300):\n",
    "        d = dataset[im]\n",
    "        if d[1] in [0, 3, 8]: \n",
    "            images.append(d[0])\n",
    "            labels.append(d[1].numpy()[0])\n",
    "#     colors = [int(i % 23) for i in labels]\n",
    "\n",
    "    for i, (name, tec) in enumerate(experiments):\n",
    "\n",
    "        tsne_model_en_2d = PCA(n_components=3, random_state=19)\n",
    "\n",
    "        n = deepcopy(net)\n",
    "        config = deepcopy(configOnline)\n",
    "\n",
    "        config.MODEL_NAME = name\n",
    "\n",
    "        if 'name' == 'gem':\n",
    "            config.EWC_IMPORTANCE = 0.5\n",
    "\n",
    "        config.EWC_TYPE = tec\n",
    "\n",
    "        trainer = Trainer(n, deepcopy(dataset), config, save_modality=2)\n",
    "\n",
    "        r = trainer.load()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            a = trainer.model.embedding(torch.stack(images, 0).to(config.DEVICE)).cpu().numpy()\n",
    "            b = tsne_model_en_2d.fit_transform(a)\n",
    "            \n",
    "#             for l in set(labels):\n",
    "#                 ix = np.where(labels == l)\n",
    "            ax[i][k].scatter(b[:, 0], b[:, 1], b[:, 2], c=labels)\n",
    "            \n",
    "        del trainer\n",
    "        del n\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
