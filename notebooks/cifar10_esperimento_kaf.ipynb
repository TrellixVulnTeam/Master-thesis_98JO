{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import NoKafnet, Kafnet\n",
    "import utils.datasetsUtils.CIFAR as CIFAR\n",
    "from utils.datasetsUtils.taskManager import SingleTargetClassificationTask, NoTask\n",
    "import configs.configClasses as configClasses\n",
    "from torchvision.transforms import transforms\n",
    "import torch\n",
    "from Trainer import Trainer\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "GeForce GTX 1050\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG PARAMETERS\n",
      "BATCH_SIZE: 64\n",
      "DEVICE: cuda\n",
      "EPOCHS: 20\n",
      "EWC_IMPORTANCE: 1000\n",
      "EWC_SAMPLE_SIZE: 250\n",
      "EWC_TYPE: <class 'networks.continual_learning.OnlineEWC'>\n",
      "GAMMA: 1.0\n",
      "IS_CONVOLUTIONAL: True\n",
      "ITERS: 1\n",
      "L1_REG: 0\n",
      "LOSS: cross_entropy\n",
      "LR: 0.001\n",
      "MODEL_NAME: ewc\n",
      "OPTIMIZER: SGD\n",
      "RUN_NAME: default\n",
      "SAVE_PATH: ./models/cifar10/kaf-vs-nokaf\n",
      "USE_EWC: True\n",
      "USE_TENSORBOARD: True\n",
      "\n",
      "CONFIG PARAMETERS\n",
      "BATCH_SIZE: 64\n",
      "DEVICE: cuda\n",
      "EPOCHS: 20\n",
      "EWC_IMPORTANCE: 1000\n",
      "EWC_SAMPLE_SIZE: 250\n",
      "EWC_TYPE: <class 'networks.continual_learning.OnlineEWC'>\n",
      "GAMMA: 1.0\n",
      "IS_CONVOLUTIONAL: True\n",
      "ITERS: 1\n",
      "L1_REG: 0\n",
      "LOSS: cross_entropy\n",
      "LR: 0.001\n",
      "MODEL_NAME: no_ewc\n",
      "OPTIMIZER: SGD\n",
      "RUN_NAME: default\n",
      "SAVE_PATH: ./models/cifar10/kaf-vs-nokaf\n",
      "USE_EWC: True\n",
      "USE_TENSORBOARD: True\n",
      "\n",
      "CONFIG PARAMETERS\n",
      "BATCH_SIZE: 64\n",
      "DEVICE: cuda\n",
      "EPOCHS: 20\n",
      "EWC_IMPORTANCE: 1000\n",
      "EWC_SAMPLE_SIZE: 250\n",
      "EWC_TYPE: <class 'networks.continual_learning.OnlineEWC'>\n",
      "GAMMA: 1.0\n",
      "IS_CONVOLUTIONAL: True\n",
      "ITERS: 1\n",
      "L1_REG: 0\n",
      "LOSS: cross_entropy\n",
      "LR: 0.001\n",
      "MODEL_NAME: no_ewc_kaf\n",
      "OPTIMIZER: SGD\n",
      "RUN_NAME: default\n",
      "SAVE_PATH: ./models/cifar10/kaf-vs-nokaf\n",
      "USE_EWC: True\n",
      "USE_TENSORBOARD: True\n",
      "\n",
      "CONFIG PARAMETERS\n",
      "BATCH_SIZE: 64\n",
      "DEVICE: cuda\n",
      "EPOCHS: 20\n",
      "EWC_IMPORTANCE: 500\n",
      "EWC_SAMPLE_SIZE: 250\n",
      "EWC_TYPE: <class 'networks.continual_learning.OnlineEWC'>\n",
      "GAMMA: 1.0\n",
      "IS_CONVOLUTIONAL: True\n",
      "ITERS: 1\n",
      "L1_REG: 0\n",
      "LOSS: cross_entropy\n",
      "LR: 0.001\n",
      "MODEL_NAME: ewc_kaf\n",
      "OPTIMIZER: SGD\n",
      "RUN_NAME: default\n",
      "SAVE_PATH: ./models/cifar10/kaf-vs-nokaf\n",
      "USE_EWC: True\n",
      "USE_TENSORBOARD: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = configClasses.OnlineLearningConfig()\n",
    "config.EPOCHS = 20\n",
    "config.L1_REG = 0\n",
    "config.IS_CONVOLUTIONAL = True\n",
    "config.SAVE_PATH = './models/cifar10/kaf-vs-nokaf'\n",
    "config.MODEL_NAME = 'ewc'\n",
    "print(config)\n",
    "\n",
    "confing_no_ewt = copy.copy(config)\n",
    "confing_no_ewt.USE_EWC = True\n",
    "confing_no_ewt.MODEL_NAME = 'no_ewc'\n",
    "print(confing_no_ewt)\n",
    "\n",
    "confing_no_ewt_kaf = copy.copy(config)\n",
    "confing_no_ewt_kaf.MODEL_NAME = 'no_ewc_kaf'\n",
    "confing_no_ewt_kaf.USE_EWC = True\n",
    "print(confing_no_ewt_kaf)\n",
    "\n",
    "confing_ewt_kaf = copy.copy(config)\n",
    "confing_ewt_kaf.MODEL_NAME = 'ewc_kaf'\n",
    "confing_ewt_kaf.EWC_IMPORTANCE = 500\n",
    "print(confing_ewt_kaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/cifar10/download\n",
      "task #0 with train 48000 and test 12000 images (label: airplane)\n",
      "task #1 with train 48000 and test 12000 images (label: automobile)\n",
      "task #2 with train 48000 and test 12000 images (label: bird)\n",
      "task #3 with train 48000 and test 12000 images (label: cat)\n",
      "task #4 with train 48000 and test 12000 images (label: deer)\n",
      "task #5 with train 48000 and test 12000 images (label: dog)\n",
      "task #6 with train 48000 and test 12000 images (label: frog)\n",
      "task #7 with train 48000 and test 12000 images (label: horse)\n",
      "task #8 with train 48000 and test 12000 images (label: ship)\n",
      "task #9 with train 48000 and test 12000 images (label: truck)\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]\n",
    ")\n",
    "\n",
    "dataset = CIFAR.Cifar10('../data/cifar10', SingleTargetClassificationTask(), download=True,\n",
    "                        force_download=False, train_split=0.8, transform=transform, target_transform=None)\n",
    "dataset.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di parametri rete classica:  171732\n",
      "Numero di parametri KAFNET:  120020\n"
     ]
    }
   ],
   "source": [
    "net_ewt = NoKafnet.CNN(dataset.tasks_number)\n",
    "net_no_ewt = copy.deepcopy(net_ewt)\n",
    "\n",
    "kaf_ewt = Kafnet.KAFCNN(dataset.tasks_number)\n",
    "kaf_no_ewt = copy.deepcopy(kaf_ewt)#Kafnet.KAFMLP(len(dataset.class_to_idx))\n",
    "\n",
    "print('Numero di parametri rete classica: ', sum([torch.numel(p) for p in net_ewt.parameters()]))\n",
    "print('Numero di parametri KAFNET: ', sum([torch.numel(p) for p in kaf_ewt.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_kaf_ewt = Trainer(kaf_ewt, copy.deepcopy(dataset), confing_ewt_kaf)\n",
    "trainer_kaf_no_ewt = Trainer(kaf_no_ewt, copy.deepcopy(dataset), confing_no_ewt_kaf)\n",
    "\n",
    "trainer_ewt = Trainer(net_ewt, copy.deepcopy(dataset), config)\n",
    "trainer_no_ewt = Trainer(net_no_ewt, copy.deepcopy(dataset), confing_no_ewt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_ewt = trainer_ewt.load()\n",
    "if not metrics_ewt:\n",
    "    metrics_ewt = trainer_ewt.all_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_no_ewt = trainer_no_ewt.load()\n",
    "if not metrics_no_ewt:\n",
    "    metrics_no_ewt = trainer_no_ewt.all_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training task (ewc) 0, epoch 1: 100%|██████████| 750/750 [02:22<00:00,  5.15it/s, loss=0.268, batch#=750]\n",
      "Testing task 0: 188it [00:15, 12.62it/s, batch#=188]\n",
      "Training task (ewc) 0, epoch 2: 100%|██████████| 750/750 [02:28<00:00,  5.43it/s, loss=0.227, batch#=750]\n",
      "Testing task 0: 188it [00:15, 12.87it/s, batch#=188]\n",
      "Training task (ewc) 0, epoch 3: 100%|██████████| 750/750 [02:37<00:00,  5.48it/s, loss=0.215, batch#=750]\n",
      "Testing task 0: 188it [00:21,  8.90it/s, batch#=188]\n",
      "Training task (ewc) 0, epoch 4: 100%|██████████| 750/750 [02:41<00:00,  6.21it/s, loss=0.207, batch#=750]\n",
      "Testing task 0: 188it [00:13, 15.30it/s, batch#=188]\n",
      "Training task (ewc) 0, epoch 5: 100%|██████████| 750/750 [01:55<00:00,  6.54it/s, loss=0.2, batch#=750]  \n",
      "Testing task 0: 188it [00:12, 15.64it/s, batch#=188]\n",
      "Training task (ewc) 0, epoch 6: 100%|██████████| 750/750 [01:57<00:00,  6.35it/s, loss=0.195, batch#=750]\n",
      "Testing task 0: 188it [00:13, 14.80it/s, batch#=188]\n",
      "Training task (ewc) 0, epoch 7: 100%|██████████| 750/750 [01:58<00:00,  6.34it/s, loss=0.19, batch#=750] \n",
      "Testing task 0: 188it [00:13, 14.81it/s, batch#=188]\n",
      "Training task (ewc) 0, epoch 8: 100%|██████████| 750/750 [01:58<00:00,  6.33it/s, loss=0.185, batch#=750]\n",
      "Testing task 0: 188it [00:13, 14.77it/s, batch#=188]\n",
      "Training task (ewc) 0, epoch 9: 100%|██████████| 750/750 [02:00<00:00,  6.18it/s, loss=0.181, batch#=750]\n",
      "Testing task 0: 188it [00:13, 14.46it/s, batch#=188]\n",
      "Training task (ewc) 0, epoch 10: 100%|██████████| 750/750 [02:01<00:00,  6.22it/s, loss=0.177, batch#=750]\n",
      "Testing task 0: 188it [00:14, 14.35it/s, batch#=188]\n",
      "Training task (ewc) 0, epoch 11: 100%|██████████| 750/750 [02:01<00:00,  6.21it/s, loss=0.174, batch#=750]\n",
      "Testing task 0: 188it [00:14, 14.44it/s, batch#=188]\n",
      "Training task (ewc) 0, epoch 12: 100%|██████████| 750/750 [02:01<00:00,  6.22it/s, loss=0.171, batch#=750]\n",
      "Testing task 0: 188it [00:14, 14.44it/s, batch#=188]\n",
      "Training task (ewc) 0, epoch 13: 100%|██████████| 750/750 [02:00<00:00,  6.22it/s, loss=0.168, batch#=750]\n",
      "Testing task 0: 188it [00:14, 14.42it/s, batch#=188]\n",
      "Training task (ewc) 0, epoch 14: 100%|██████████| 750/750 [02:01<00:00,  6.21it/s, loss=0.165, batch#=750]\n",
      "Testing task 0: 188it [00:14, 14.39it/s, batch#=188]\n",
      "Training task (ewc) 0, epoch 15: 100%|██████████| 750/750 [02:01<00:00,  6.21it/s, loss=0.163, batch#=750]\n",
      "Testing task 0: 188it [00:14, 14.45it/s, batch#=188]\n",
      "Training task (ewc) 0, epoch 16: 100%|██████████| 750/750 [02:01<00:00,  6.18it/s, loss=0.16, batch#=750] \n",
      "Testing task 0: 188it [00:13, 14.45it/s, batch#=188]\n",
      "Training task (ewc) 0, epoch 17: 100%|██████████| 750/750 [02:01<00:00,  6.21it/s, loss=0.158, batch#=750]\n",
      "Testing task 0: 188it [00:14, 14.39it/s, batch#=188]\n",
      "Training task (ewc) 0, epoch 18: 100%|██████████| 750/750 [02:00<00:00,  6.21it/s, loss=0.157, batch#=750]\n",
      "Testing task 0: 188it [00:14, 14.34it/s, batch#=188]\n",
      "Training task (ewc) 0, epoch 19: 100%|██████████| 750/750 [02:01<00:00,  6.21it/s, loss=0.154, batch#=750]\n",
      "Testing task 0: 188it [00:14, 14.40it/s, batch#=188]\n",
      "Training task (ewc) 0, epoch 20: 100%|██████████| 750/750 [02:01<00:00,  6.18it/s, loss=0.152, batch#=750]\n",
      "Testing task 0: 188it [00:14, 14.36it/s, batch#=188]\n",
      "Testing task 1: 188it [00:14, 14.47it/s, batch#=188]\n",
      "Testing task 2: 188it [00:14, 14.39it/s, batch#=188]\n",
      "Testing task 3: 188it [00:14, 14.41it/s, batch#=188]\n",
      "Testing task 4: 188it [00:13, 14.48it/s, batch#=188]\n",
      "Testing task 5: 188it [00:13, 14.38it/s, batch#=188]\n",
      "Testing task 6: 188it [00:13, 14.40it/s, batch#=188]\n",
      "Testing task 7: 188it [00:14, 14.40it/s, batch#=188]\n",
      "Testing task 8: 188it [00:14, 14.41it/s, batch#=188]\n",
      "Testing task 9: 188it [00:14, 14.47it/s, batch#=188]\n",
      "Training task (ewc) 1, epoch 1: 100%|██████████| 750/750 [02:02<00:00,  6.27it/s, loss=0.274, batch#=750]\n",
      "Testing task 1: 188it [00:13, 15.01it/s, batch#=188]\n",
      "Testing task 0: 188it [00:13, 14.45it/s, batch#=188]\n",
      "Training task (ewc) 1, epoch 2: 100%|██████████| 750/750 [02:02<00:00,  6.15it/s, loss=0.36, batch#=750] \n",
      "Testing task 1: 188it [00:13, 14.44it/s, batch#=188]\n",
      "Testing task 0: 188it [00:14, 14.43it/s, batch#=188]\n",
      "Training task (ewc) 1, epoch 3: 100%|██████████| 750/750 [02:01<00:00,  6.27it/s, loss=0.328, batch#=750]\n",
      "Testing task 1: 188it [00:13, 14.43it/s, batch#=188]\n",
      "Testing task 0: 188it [00:14, 14.40it/s, batch#=188]\n",
      "Training task (ewc) 1, epoch 4: 100%|██████████| 750/750 [02:02<00:00,  6.15it/s, loss=0.319, batch#=750]\n",
      "Testing task 1: 188it [00:14, 14.43it/s, batch#=188]\n",
      "Testing task 0: 188it [00:14, 14.41it/s, batch#=188]\n",
      "Training task (ewc) 1, epoch 5: 100%|██████████| 750/750 [02:02<00:00,  6.12it/s, loss=0.315, batch#=750]\n",
      "Testing task 1: 188it [00:14, 14.41it/s, batch#=188]\n",
      "Testing task 0: 188it [00:14, 14.45it/s, batch#=188]\n",
      "Training task (ewc) 1, epoch 6: 100%|██████████| 750/750 [02:02<00:00,  6.13it/s, loss=0.312, batch#=750]\n",
      "Testing task 1: 188it [00:14, 14.39it/s, batch#=188]\n",
      "Testing task 0: 188it [00:14, 14.35it/s, batch#=188]\n",
      "Training task (ewc) 1, epoch 7: 100%|██████████| 750/750 [02:03<00:00,  6.15it/s, loss=0.31, batch#=750] \n",
      "Testing task 1: 188it [00:14, 14.47it/s, batch#=188]\n",
      "Testing task 0: 188it [00:14, 14.35it/s, batch#=188]\n",
      "Training task (ewc) 1, epoch 8: 100%|██████████| 750/750 [02:02<00:00,  6.14it/s, loss=0.308, batch#=750]\n",
      "Testing task 1: 188it [00:14, 14.36it/s, batch#=188]\n",
      "Testing task 0: 188it [00:14, 14.41it/s, batch#=188]\n",
      "Training task (ewc) 1, epoch 9: 100%|██████████| 750/750 [02:02<00:00,  6.15it/s, loss=0.307, batch#=750]\n",
      "Testing task 1: 188it [00:14, 14.36it/s, batch#=188]\n",
      "Testing task 0: 188it [00:14, 14.36it/s, batch#=188]\n",
      "Training task (ewc) 1, epoch 10: 100%|██████████| 750/750 [02:02<00:00,  6.09it/s, loss=0.305, batch#=750]\n",
      "Testing task 1: 188it [00:14, 14.35it/s, batch#=188]\n",
      "Testing task 0: 188it [00:14, 14.38it/s, batch#=188]\n",
      "Training task (ewc) 1, epoch 11: 100%|██████████| 750/750 [02:02<00:00,  6.15it/s, loss=0.304, batch#=750]\n",
      "Testing task 1: 188it [00:14, 14.41it/s, batch#=188]\n",
      "Testing task 0: 188it [00:13, 14.43it/s, batch#=188]\n",
      "Training task (ewc) 1, epoch 12: 100%|██████████| 750/750 [02:02<00:00,  6.00it/s, loss=0.304, batch#=750]\n",
      "Testing task 1: 188it [00:14, 14.40it/s, batch#=188]\n",
      "Testing task 0: 188it [00:14, 14.37it/s, batch#=188]\n",
      "Training task (ewc) 1, epoch 13: 100%|██████████| 750/750 [02:02<00:00,  6.14it/s, loss=0.303, batch#=750]\n",
      "Testing task 1: 188it [00:14, 14.44it/s, batch#=188]\n",
      "Testing task 0: 188it [00:14, 14.39it/s, batch#=188]\n",
      "Training task (ewc) 1, epoch 14: 100%|██████████| 750/750 [02:02<00:00,  6.14it/s, loss=0.302, batch#=750]\n",
      "Testing task 1: 188it [00:13, 14.47it/s, batch#=188]\n",
      "Testing task 0: 188it [00:14, 14.44it/s, batch#=188]\n",
      "Training task (ewc) 1, epoch 15: 100%|██████████| 750/750 [02:02<00:00,  6.15it/s, loss=0.301, batch#=750]\n",
      "Testing task 1: 188it [00:14, 14.41it/s, batch#=188]\n",
      "Testing task 0: 188it [00:13, 14.45it/s, batch#=188]\n",
      "Training task (ewc) 1, epoch 16: 100%|██████████| 750/750 [02:02<00:00,  6.15it/s, loss=0.301, batch#=750]\n",
      "Testing task 1: 188it [00:14, 14.47it/s, batch#=188]\n",
      "Testing task 0: 188it [00:13, 14.45it/s, batch#=188]\n",
      "Training task (ewc) 1, epoch 17: 100%|██████████| 750/750 [02:02<00:00,  6.15it/s, loss=0.3, batch#=750]  \n",
      "Testing task 1: 188it [00:13, 14.44it/s, batch#=188]\n",
      "Testing task 0: 188it [00:14, 14.40it/s, batch#=188]\n",
      "Training task (ewc) 1, epoch 18: 100%|██████████| 750/750 [02:02<00:00,  6.15it/s, loss=0.299, batch#=750]\n",
      "Testing task 1: 188it [00:14, 14.41it/s, batch#=188]\n",
      "Testing task 0: 188it [00:14, 14.45it/s, batch#=188]\n",
      "Training task (ewc) 1, epoch 19: 100%|██████████| 750/750 [02:02<00:00,  6.14it/s, loss=0.299, batch#=750]\n",
      "Testing task 1: 188it [00:14, 14.37it/s, batch#=188]\n",
      "Testing task 0: 188it [00:14, 14.44it/s, batch#=188]\n",
      "Training task (ewc) 1, epoch 20: 100%|██████████| 750/750 [02:02<00:00,  6.15it/s, loss=0.298, batch#=750]\n",
      "Testing task 1: 188it [00:14, 14.26it/s, batch#=188]\n",
      "Testing task 0: 188it [00:14, 14.38it/s, batch#=188]\n",
      "Testing task 2: 188it [00:14, 14.48it/s, batch#=188]\n",
      "Testing task 3: 188it [00:13, 14.47it/s, batch#=188]\n",
      "Testing task 4: 188it [00:14, 14.45it/s, batch#=188]\n",
      "Testing task 5: 188it [00:14, 14.39it/s, batch#=188]\n",
      "Testing task 6: 188it [00:14, 14.43it/s, batch#=188]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing task 7: 188it [00:13, 14.65it/s, batch#=188]\n",
      "Testing task 8: 188it [00:13, 14.67it/s, batch#=188]\n",
      "Testing task 9: 188it [00:13, 14.70it/s, batch#=188]\n",
      "Training task (ewc) 2, epoch 1: 100%|██████████| 750/750 [02:00<00:00,  6.26it/s, loss=0.313, batch#=750]\n",
      "Testing task 2: 188it [00:13, 14.65it/s, batch#=188]\n",
      "Testing task 0: 188it [00:13, 14.66it/s, batch#=188]\n",
      "Testing task 1: 188it [00:13, 14.68it/s, batch#=188]\n",
      "Training task (ewc) 2, epoch 2: 100%|██████████| 750/750 [02:00<00:00,  6.23it/s, loss=0.381, batch#=750]\n",
      "Testing task 2: 188it [00:13, 14.60it/s, batch#=188]\n",
      "Testing task 0: 188it [00:13, 14.60it/s, batch#=188]\n",
      "Testing task 1: 188it [00:13, 14.68it/s, batch#=188]\n",
      "Training task (ewc) 2, epoch 3: 100%|██████████| 750/750 [02:00<00:00,  6.26it/s, loss=0.367, batch#=750]\n",
      "Testing task 2: 188it [00:13, 14.68it/s, batch#=188]\n",
      "Testing task 0: 188it [00:13, 14.66it/s, batch#=188]\n",
      "Testing task 1: 188it [00:13, 14.60it/s, batch#=188]\n",
      "Training task (ewc) 2, epoch 4: 100%|██████████| 750/750 [02:21<00:00,  5.42it/s, loss=0.363, batch#=750]\n",
      "Testing task 2: 188it [00:15, 13.18it/s, batch#=188]\n",
      "Testing task 0: 188it [00:15, 13.35it/s, batch#=188]\n",
      "Testing task 1: 188it [00:15, 13.24it/s, batch#=188]\n",
      "Training task (ewc) 2, epoch 5: 100%|██████████| 750/750 [02:18<00:00,  5.40it/s, loss=0.361, batch#=750]\n",
      "Testing task 2: 188it [00:15, 13.22it/s, batch#=188]\n",
      "Testing task 0: 188it [00:15, 13.19it/s, batch#=188]\n",
      "Testing task 1: 188it [00:15, 13.10it/s, batch#=188]\n",
      "Training task (ewc) 2, epoch 6: 100%|██████████| 750/750 [02:18<00:00,  5.38it/s, loss=0.359, batch#=750]\n",
      "Testing task 2: 188it [00:15, 13.22it/s, batch#=188]\n",
      "Testing task 0: 188it [00:15, 13.06it/s, batch#=188]\n",
      "Testing task 1: 188it [00:15, 13.18it/s, batch#=188]\n",
      "Training task (ewc) 2, epoch 7: 100%|██████████| 750/750 [02:19<00:00,  5.35it/s, loss=0.358, batch#=750]\n",
      "Testing task 2: 188it [00:15, 12.89it/s, batch#=188]\n",
      "Testing task 0: 188it [00:15, 12.97it/s, batch#=188]\n",
      "Testing task 1: 188it [00:15, 12.93it/s, batch#=188]\n",
      "Training task (ewc) 2, epoch 8: 100%|██████████| 750/750 [02:20<00:00,  5.38it/s, loss=0.357, batch#=750]\n",
      "Testing task 2: 188it [00:15, 12.96it/s, batch#=188]\n",
      "Testing task 0: 188it [00:15, 12.97it/s, batch#=188]\n",
      "Testing task 1: 188it [00:15, 12.90it/s, batch#=188]\n",
      "Training task (ewc) 2, epoch 9: 100%|██████████| 750/750 [02:20<00:00,  5.32it/s, loss=0.356, batch#=750]\n",
      "Testing task 2: 188it [00:15, 12.90it/s, batch#=188]\n",
      "Testing task 0: 188it [00:15, 12.88it/s, batch#=188]\n",
      "Testing task 1: 188it [00:15, 13.02it/s, batch#=188]\n",
      "Training task (ewc) 2, epoch 10: 100%|██████████| 750/750 [02:19<00:00,  5.36it/s, loss=0.355, batch#=750]\n",
      "Testing task 2: 188it [00:15, 13.03it/s, batch#=188]\n",
      "Testing task 0: 188it [00:15, 13.11it/s, batch#=188]\n",
      "Testing task 1: 188it [00:15, 12.95it/s, batch#=188]\n",
      "Training task (ewc) 2, epoch 11:  88%|████████▊ | 660/750 [02:04<00:17,  5.03it/s, loss=0.355, batch#=660]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a57a604fcc8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmetrics_kaf_ewt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer_kaf_ewt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmetrics_kaf_ewt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmetrics_kaf_ewt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer_kaf_ewt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_tasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/jary/DATA/Uni/tesi/codice/Trainer.py\u001b[0m in \u001b[0;36mall_tasks\u001b[0;34m(self, limit)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/jary/DATA/Uni/tesi/codice/Trainer.py\u001b[0m in \u001b[0;36mepoch\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0mepoch_loss_full\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metrics_kaf_ewt = trainer_kaf_ewt.load()\n",
    "if not metrics_kaf_ewt:\n",
    "    metrics_kaf_ewt = trainer_kaf_ewt.all_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_kaf_no_ewt = trainer_kaf_no_ewt.load()\n",
    "if not metrics_kaf_no_ewt:\n",
    "    metrics_kaf_no_ewt = trainer_kaf_no_ewt.all_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_task = len(metrics_ewt['tasks'])\n",
    "tot_epochs = 0\n",
    "\n",
    "print('No ewc', metrics_no_ewt['metrics'])\n",
    "print('Online ewc', metrics_ewt['metrics'])\n",
    "\n",
    "print('KAF no ewc', metrics_kaf_no_ewt['metrics'])\n",
    "print('KAF online ewc', metrics_kaf_ewt['metrics'])\n",
    "\n",
    "for k, v in metrics_no_ewt['tasks'].items():\n",
    "    tot_epochs = max(tot_epochs, len(v['accuracy']))\n",
    "             \n",
    "for k, v in metrics_ewt['tasks'].items():\n",
    "    tot_epochs = max(tot_epochs, len(v['accuracy']))\n",
    "      \n",
    "fig = plt.figure(figsize=(12, 24))\n",
    "\n",
    "ax = None\n",
    "for i, task in enumerate(metrics_ewt['tasks'].keys()):\n",
    "        \n",
    "    ewt = metrics_ewt['tasks'][task]\n",
    "    no_ewt = metrics_no_ewt['tasks'][task]\n",
    "    \n",
    "    kaf_ewt = metrics_kaf_ewt['tasks'][task]\n",
    "    kaf_no_ewt = metrics_kaf_no_ewt['tasks'][task]\n",
    "    \n",
    "\n",
    "    x = range(tot_epochs-len(ewt['accuracy']), tot_epochs)\n",
    "\n",
    "    ax = fig.add_subplot(n_task, 1, i+1, sharex=ax) \n",
    "    \n",
    "    ax.plot(x, ewt['f1'], label='ewt')\n",
    "    ax.plot(x, no_ewt['f1'], label='no ewt')\n",
    "    ax.plot(x, kaf_ewt['f1'], label='kaf ewt')\n",
    "    ax.plot(x, kaf_no_ewt['f1'], label='kaf no ewt')\n",
    "    \n",
    "    ax.set_xticks(list(range(0, tot_epochs, 2))+[39],minor=False)\n",
    "    \n",
    "    ax.set_title(\"Task {}\".format(task))\n",
    "    ax.legend(loc=\"lower left\")\n",
    "    ax.grid(True, axis='x')\n",
    "    \n",
    "fig.subplots_adjust(hspace=0.01)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
